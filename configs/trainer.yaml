trainer:
  _target_: pytorch_lightning.Trainer

  max_epochs: 1000
  log_every_n_steps: 50
  num_sanity_val_steps: 0
  check_val_every_n_epoch: 1
  accelerator: gpu
  devices: 1

  callbacks:
  # a. last ckpt 回调
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      filename: latest-{epoch}
      every_n_epochs: 1
      save_top_k: 1
      save_last: true
  # b. every 100 ckpt 回调
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      filename: latest-{epoch}
      monitor: step
      mode: max
      every_n_epochs: 100
      save_top_k: -1
      save_last: false
    # c. best val_loss ckpt 回调
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      # 如果不指定 dirpath, 它会自动使用 logger.log_dir (即 outputs/experiment_name/version_X/checkpoints)
      # 这正是我们想要的！
      monitor: "val_loss_epoch" # 需要监控的指标
      mode: "min"
      save_top_k: 3
      save_last: true # 保存最后一个 epoch 的 checkpoint
      filename: "e_{epoch:03d}-val_{val:.2f}" # checkpoint 文件名格式
    - _target_: src.callback.progress.ProgressLogger
      precision: 3
    - _target_: src.callback.tqdmbar.TQDMProgressBar

  logger:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ${hydra:runtime.output_dir}
    # 实验名称，可以根据模型和数据动态生成
    name: ""
    # version 会自动递增 (version_0, version_1, ...)
